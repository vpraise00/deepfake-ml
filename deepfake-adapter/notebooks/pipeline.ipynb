{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "dc9c6105",
   "metadata": {},
   "source": [
    "# FFPP Deepfake Detection Pipeline (80:20 → 7:1 → 5-Fold → Test)\n",
    "\n",
    "이 노트북은 “alltype” 폴더 구조를 사용하는 FaceForensics++ 데이터셋을 기반으로,  \n",
    "아래 절차를 순차적으로 수행합니다:\n",
    "\n",
    "1. **데이터 로드 및 전처리 확인**  \n",
    "   - `configs/ffpp_c23.yaml`을 기준으로 데이터 경로(`dataset_root/version/alltype`) 설정  \n",
    "   - `FFPPFrameDataset`을 사용해 모든 프레임 이미지와 레이블(Real/Fake) 로드  \n",
    "   - 전처리 파이프라인(`get_ffpp_transforms`)이 정상 작동하는지 샘플 이미지 시각화\n",
    "\n",
    "2. **Train/Val/Test 분할 및 5-Fold Cross-Validation 학습**  \n",
    "   - 전체(alltype) 데이터 → 80% Train+Val / 20% Test 분할 (stratify)  \n",
    "   - Train+Val(80%) → Train(70%) / Val(10%) 분할  \n",
    "   - Train(70%) → 5-Fold Cross-Validation (Train:Val = 4:1)  \n",
    "   - 각 Fold마다 학습 및 검증 수행 → Fold별 “best.pth” 저장\n",
    "\n",
    "3. **Test(20%) 성능 평가**  \n",
    "   - 마지막(5번째) Fold의 “best.pth” 모델을 불러와 Test 데이터셋에 대해 AUC/EER 계산\n",
    "\n",
    "4. **단일 비디오 추론 (Inference)**  \n",
    "   - Test 평가 이후, 새로운 비디오 파일을 “영상 → 프레임”으로 분할  \n",
    "   - 각 프레임별 Deepfake 확률 예측 → JSON으로 저장  \n",
    "   - (선택) 상위 확률 프레임 5개 시각화\n",
    "\n",
    "---\n",
    "\n",
    "**사전 준비**  \n",
    "1. `deepfake_adapter_ffpp/` 디렉터리 구조가 다음과 같아야 합니다:\n",
    "    ```\n",
    "    deepfake_adapter_ffpp/\n",
    "    ├── configs/\n",
    "    │   ├── ffpp_c23.yaml\n",
    "    │   └── ffpp_c40.yaml\n",
    "    ├── data/\n",
    "    │   └── README.md\n",
    "    ├── notebooks/\n",
    "    │   └── 01_pipeline.ipynb\n",
    "    ├── src/\n",
    "    │   ├── datasets/ffpp_dataset.py\n",
    "    │   ├── models/\n",
    "    │   │   ├── backbone.py\n",
    "    │   │   ├── adapters.py\n",
    "    │   │   └── deepfake_adapter.py\n",
    "    │   ├── utils/\n",
    "    │   │   ├── transforms.py\n",
    "    │   │   └── metrics.py\n",
    "    │   └── scripts/\n",
    "    │       ├── train.py\n",
    "    │       └── predict.py\n",
    "    ├── outputs/\n",
    "    │   ├── checkpoints/\n",
    "    │   └── logs/\n",
    "    ├── requirements.txt\n",
    "    └── README.md\n",
    "    ```\n",
    "2. `configs/ffpp_c23.yaml`의 `dataset_root` 경로를 본인 환경에 맞게 수정하세요.  \n",
    "   예: `\"dataset_root\": \"D:/Files/data/FaceForensicspp_RECCE\"`  \n",
    "3. 필요한 라이브러리를 설치합니다:  \n",
    "   ```bash\n",
    "   pip install -r requirements.txt\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6819d221",
   "metadata": {},
   "source": [
    "- `configs/ffpp_c23.yaml`의 `dataset_root` 경로를 본인 환경에 맞게 수정하세요.  \n",
    "- 필요한 라이브러리(`torch`, `timm`, `yaml`, `opencv-python`, `torchvision`, `sklearn` 등)를 설치합니다:  \n",
    "```bash\n",
    "pip install -r requirements.txt\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6561ea7a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Device: cuda\n",
      "Config: {'dataset_root': 'D:/My Project/Deepfake/df_train/data/FaceForensicspp_RECCE', 'version': 'c23_frames', 'input_size': 224, 'batch_size': 32, 'num_workers': 4, 'model': {'backbone': 'vit_base_patch16_224', 'adapter_dim': 128, 'num_classes': 2}, 'optimizer': {'type': 'adamw', 'lr': '1e-4', 'weight_decay': '1e-5'}, 'scheduler': {'type': 'cosine', 'T_max': 50}, 'training': {'epochs': 15, 'k_folds': 5}, 'output_dir': 'outputs/checkpoints/ffpp_c23', 'log_dir': 'outputs/logs/ffpp_c23'}\n"
     ]
    }
   ],
   "source": [
    "# 셀 2: 라이브러리 임포트 및 설정 불러오기\n",
    "\n",
    "import os\n",
    "import yaml\n",
    "import random\n",
    "import json\n",
    "import numpy as np\n",
    "import torch\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "from PIL import Image\n",
    "import cv2\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# src/ 경로를 import 경로에 추가\n",
    "import sys\n",
    "sys.path.append(os.path.abspath(os.path.join(os.getcwd(), \"..\")))\n",
    "\n",
    "# 사용자 정의 모듈\n",
    "from src.datasets.ffpp_dataset import FFPPFrameDataset\n",
    "from src.models.deepfake_adapter import DeepfakeAdapter\n",
    "from src.utils.transforms import get_ffpp_transforms\n",
    "from src.utils.metrics import compute_auc, compute_eer\n",
    "\n",
    "# --- 설정(Config) 로드 ---\n",
    "config_path = \"../configs/ffpp_c23.yaml\"  # 노트북이 notebooks/ 폴더 안에 있으므로 상위 폴더 참조\n",
    "with open(config_path, \"r\") as f:\n",
    "    cfg = yaml.safe_load(f)\n",
    "\n",
    "# 재현성(Seed) 고정 함수\n",
    "def set_seed(seed: int = 42):\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed_all(seed)\n",
    "\n",
    "# 디바이스 설정 (GPU 사용 우선)\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Device: {device}\")\n",
    "print(\"Config:\", cfg)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "445d0ad6",
   "metadata": {},
   "source": [
    "## 데이터셋 로드 및 전처리 확인\n",
    "\n",
    "- `FFPPFrameDataset`을 이용해 **alltype** 데이터 전체를 로드합니다.\n",
    "- `get_ffpp_transforms(cfg[\"input_size\"])`으로 이미지가 Resize → ToTensor → Normalize 되는지 확인합니다.\n",
    "- 샘플 이미지를 시각화하여 전처리 결과를 눈으로 확인해 봅니다.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a70f3153",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total sampled samples (alltype): 430850\n",
      "Label counts (샘플링 후): {np.int64(0): np.int64(247008), np.int64(1): np.int64(183842)}  (0=real, 1=fake)\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "File \u001b[1;32mc:\\Users\\vprai\\anaconda3\\envs\\dfML\\lib\\pathlib.py:658\u001b[0m, in \u001b[0;36mPurePath._cparts\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    657\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 658\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_cached_cparts\u001b[49m\n\u001b[0;32m    659\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mAttributeError\u001b[39;00m:\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'WindowsPath' object has no attribute '_cached_cparts'",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[2], line 24\u001b[0m\n\u001b[0;32m     20\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mLabel counts (샘플링 후): \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mlabel_counts\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m  (0=real, 1=fake)\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m     22\u001b[0m \u001b[38;5;66;03m# (C) 임시 DataLoader 구성 — 배치 형태 확인용\u001b[39;00m\n\u001b[0;32m     23\u001b[0m \u001b[38;5;66;03m# 여기서는 학습용 transform만 적용해서 한번 이미지가 어떻게 변하는지 시각화해보고 싶다면:\u001b[39;00m\n\u001b[1;32m---> 24\u001b[0m vis_dataset \u001b[38;5;241m=\u001b[39m \u001b[43mFFPPFrameDataset\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m     25\u001b[0m \u001b[43m    \u001b[49m\u001b[43mconfig_path\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     26\u001b[0m \u001b[43m    \u001b[49m\u001b[43mframe_interval\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m5\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m     27\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtransform\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtrain_transform\u001b[49m\n\u001b[0;32m     28\u001b[0m \u001b[43m)\u001b[49m\n\u001b[0;32m     29\u001b[0m temp_loader \u001b[38;5;241m=\u001b[39m DataLoader(\n\u001b[0;32m     30\u001b[0m     vis_dataset,\n\u001b[0;32m     31\u001b[0m     batch_size\u001b[38;5;241m=\u001b[39mcfg[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mbatch_size\u001b[39m\u001b[38;5;124m\"\u001b[39m],\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     34\u001b[0m     pin_memory\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[0;32m     35\u001b[0m )\n\u001b[0;32m     37\u001b[0m \u001b[38;5;66;03m# (D) 첫 번째 배치 가져와서 이미지 텐서 모양 확인\u001b[39;00m\n",
      "File \u001b[1;32mg:\\다른 컴퓨터\\Desktop\\학교자료\\학교 자료\\연구\\졸업프로젝트\\종합설계1\\Code\\deepfake-ml\\deepfake-ml\\deepfake-adapter\\src\\datasets\\ffpp_dataset.py:78\u001b[0m, in \u001b[0;36mFFPPFrameDataset.__init__\u001b[1;34m(self, config_path, frame_interval, transform, indices)\u001b[0m\n\u001b[0;32m     75\u001b[0m     \u001b[38;5;28;01mcontinue\u001b[39;00m\n\u001b[0;32m     77\u001b[0m \u001b[38;5;66;03m# 3-1) 모든 프레임 파일을 정렬하여 가져온 뒤\u001b[39;00m\n\u001b[1;32m---> 78\u001b[0m img_files \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43msorted\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mvideo_dir\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mglob\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m*.jpg\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;241m+\u001b[39m \u001b[38;5;28msorted\u001b[39m(video_dir\u001b[38;5;241m.\u001b[39mglob(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m*.png\u001b[39m\u001b[38;5;124m\"\u001b[39m))\n\u001b[0;32m     79\u001b[0m \u001b[38;5;66;03m# 3-2) interval 단위로 샘플링\u001b[39;00m\n\u001b[0;32m     80\u001b[0m sampled \u001b[38;5;241m=\u001b[39m img_files[:: \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mframe_interval]\n",
      "File \u001b[1;32mc:\\Users\\vprai\\anaconda3\\envs\\dfML\\lib\\pathlib.py:678\u001b[0m, in \u001b[0;36mPurePath.__lt__\u001b[1;34m(self, other)\u001b[0m\n\u001b[0;32m    676\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(other, PurePath) \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_flavour \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m other\u001b[38;5;241m.\u001b[39m_flavour:\n\u001b[0;32m    677\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mNotImplemented\u001b[39m\n\u001b[1;32m--> 678\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_cparts\u001b[49m \u001b[38;5;241m<\u001b[39m other\u001b[38;5;241m.\u001b[39m_cparts\n",
      "File \u001b[1;32mc:\\Users\\vprai\\anaconda3\\envs\\dfML\\lib\\pathlib.py:660\u001b[0m, in \u001b[0;36mPurePath._cparts\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    658\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_cached_cparts\n\u001b[0;32m    659\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mAttributeError\u001b[39;00m:\n\u001b[1;32m--> 660\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_cached_cparts \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_flavour\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcasefold_parts\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_parts\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    661\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_cached_cparts\n",
      "File \u001b[1;32mc:\\Users\\vprai\\anaconda3\\envs\\dfML\\lib\\pathlib.py:190\u001b[0m, in \u001b[0;36m_WindowsFlavour.casefold_parts\u001b[1;34m(self, parts)\u001b[0m\n\u001b[0;32m    189\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mcasefold_parts\u001b[39m(\u001b[38;5;28mself\u001b[39m, parts):\n\u001b[1;32m--> 190\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m [p\u001b[38;5;241m.\u001b[39mlower() \u001b[38;5;28;01mfor\u001b[39;00m p \u001b[38;5;129;01min\u001b[39;00m parts]\n",
      "File \u001b[1;32mc:\\Users\\vprai\\anaconda3\\envs\\dfML\\lib\\pathlib.py:190\u001b[0m, in \u001b[0;36m<listcomp>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m    189\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mcasefold_parts\u001b[39m(\u001b[38;5;28mself\u001b[39m, parts):\n\u001b[1;32m--> 190\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m [\u001b[43mp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlower\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mfor\u001b[39;00m p \u001b[38;5;129;01min\u001b[39;00m parts]\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# 셀 3: 데이터셋 로드 및 전처리 확인 (샘플링, 증강 포함)\n",
    "\n",
    "set_seed(42)\n",
    "\n",
    "# (A) 학습/검증용 transform 불러오기\n",
    "train_transform, val_transform = get_ffpp_transforms(cfg[\"input_size\"])\n",
    "\n",
    "# (B) 전체 데이터셋 로드 (샘플링 interval=5, transform=None → 단순 count용)\n",
    "full_dataset = FFPPFrameDataset(\n",
    "    config_path,\n",
    "    frame_interval=5,   # 5프레임당 1샘플 사용\n",
    "    transform=None      # 단순 통계용이므로 transform은 생략\n",
    ")\n",
    "print(f\"Total sampled samples (alltype): {len(full_dataset)}\")\n",
    "\n",
    "# 레이블별 개수 세기\n",
    "labels_array = np.array(full_dataset.labels)\n",
    "unique, counts = np.unique(labels_array, return_counts=True)\n",
    "label_counts = dict(zip(unique, counts))\n",
    "print(f\"Label counts (샘플링 후): {label_counts}  (0=real, 1=fake)\")\n",
    "\n",
    "# (C) 임시 DataLoader 구성 — 배치 형태 확인용\n",
    "# 여기서는 학습용 transform만 적용해서 한번 이미지가 어떻게 변하는지 시각화해보고 싶다면:\n",
    "vis_dataset = FFPPFrameDataset(\n",
    "    config_path,\n",
    "    frame_interval=5,\n",
    "    transform=train_transform\n",
    ")\n",
    "temp_loader = DataLoader(\n",
    "    vis_dataset,\n",
    "    batch_size=cfg[\"batch_size\"],\n",
    "    shuffle=True,\n",
    "    num_workers=cfg[\"num_workers\"],\n",
    "    pin_memory=True\n",
    ")\n",
    "\n",
    "# (D) 첫 번째 배치 가져와서 이미지 텐서 모양 확인\n",
    "images, labels = next(iter(temp_loader))\n",
    "print(\"Batch images shape:\", images.shape)  # [batch_size, 3, input_size, input_size]\n",
    "print(\"Batch labels shape:\", labels.shape)  # [batch_size]\n",
    "\n",
    "# (E) 샘플 이미지 시각화 (정규화 해제 후 표시)\n",
    "def imshow_tensor(img_tensor, mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]):\n",
    "    \"\"\"정규화된 텐서를 이미지로 되돌려 Matplotlib으로 표시\"\"\"\n",
    "    img = img_tensor.clone().cpu().numpy()\n",
    "    for c in range(3):\n",
    "        img[c] = img[c] * std[c] + mean[c]\n",
    "    img = np.clip(img, 0, 1)\n",
    "    img = np.transpose(img, (1, 2, 0))  # (H, W, C)\n",
    "    plt.imshow(img)\n",
    "    plt.axis(\"off\")\n",
    "\n",
    "fig, axes = plt.subplots(1, 4, figsize=(12, 4))\n",
    "for i in range(4):\n",
    "    imshow_tensor(images[i])\n",
    "    axes[i].set_title(f\"Label: {labels[i].item()}\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b67aab1",
   "metadata": {},
   "source": [
    "## Train/Val/Test 분할 (80:20 → 7:1) + 5-Fold 학습/검증 + Test 평가\n",
    "\n",
    "1. 전체(alltype) 데이터 → Train+Val(80%) / Test(20%) 분할 (stratify)\n",
    "2. Train+Val(80%) → Train(70%) / Val(10%) 분할 (stratify)\n",
    "3. Train(70%)에 대해 **5-Fold Cross-Validation** 수행\n",
    "   - 각 Fold마다 Train/Val 로더 구성 → 학습 및 검증 → Fold별 최적 모델 저장\n",
    "4. 마지막(5번째) Fold의 모델을 불러와 Test(20%) 평가 (AUC/EER)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5323f892",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total sampled samples (alltype): 430850\n",
      "전체 레이블 분포: {np.int64(0): np.int64(247008), np.int64(1): np.int64(183842)} (0=real,1=fake)\n",
      "Train+Val count: 344680, Test count: 86170\n",
      "Train count: 301595, Val count: 43085, Test count: 86170\n",
      "Train set 클래스별 카운트: {np.int64(0): np.int64(172905), np.int64(1): np.int64(128690)}, 가중치: {0: np.float64(1.7442815418871636), 1: np.float64(2.3435775895562982)}\n",
      "\n",
      "--- Fold 1/5 ---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                           \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Fold 1/5 | Epoch 1/15] Train Loss: 0.3554  Val AUC: 0.9815  Val EER: 0.0767\n",
      "→ New best model for Fold 1 saved at outputs/checkpoints/ffpp_c23\\fold_1\\best.pth\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                           \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Fold 1/5 | Epoch 2/15] Train Loss: 0.1869  Val AUC: 0.9932  Val EER: 0.0454\n",
      "→ New best model for Fold 1 saved at outputs/checkpoints/ffpp_c23\\fold_1\\best.pth\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                           \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Fold 1/5 | Epoch 3/15] Train Loss: 0.1368  Val AUC: 0.9964  Val EER: 0.0332\n",
      "→ New best model for Fold 1 saved at outputs/checkpoints/ffpp_c23\\fold_1\\best.pth\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                           \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Fold 1/5 | Epoch 4/15] Train Loss: 0.1110  Val AUC: 0.9979  Val EER: 0.0260\n",
      "→ New best model for Fold 1 saved at outputs/checkpoints/ffpp_c23\\fold_1\\best.pth\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                           \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Fold 1/5 | Epoch 5/15] Train Loss: 0.0936  Val AUC: 0.9983  Val EER: 0.0235\n",
      "→ New best model for Fold 1 saved at outputs/checkpoints/ffpp_c23\\fold_1\\best.pth\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                           \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Fold 1/5 | Epoch 6/15] Train Loss: 0.0812  Val AUC: 0.9986  Val EER: 0.0218\n",
      "→ New best model for Fold 1 saved at outputs/checkpoints/ffpp_c23\\fold_1\\best.pth\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                           \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Fold 1/5 | Epoch 7/15] Train Loss: 0.0724  Val AUC: 0.9988  Val EER: 0.0202\n",
      "→ New best model for Fold 1 saved at outputs/checkpoints/ffpp_c23\\fold_1\\best.pth\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                           \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Fold 1/5 | Epoch 8/15] Train Loss: 0.0673  Val AUC: 0.9992  Val EER: 0.0175\n",
      "→ New best model for Fold 1 saved at outputs/checkpoints/ffpp_c23\\fold_1\\best.pth\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                           \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Fold 1/5 | Epoch 9/15] Train Loss: 0.0617  Val AUC: 0.9991  Val EER: 0.0186\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                          \r"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[3], line 144\u001b[0m\n\u001b[0;32m    141\u001b[0m     loss\u001b[38;5;241m.\u001b[39mbackward()\n\u001b[0;32m    142\u001b[0m     optimizer\u001b[38;5;241m.\u001b[39mstep()\n\u001b[1;32m--> 144\u001b[0m     running_loss \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[43mloss\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mitem\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;241m*\u001b[39m images\u001b[38;5;241m.\u001b[39msize(\u001b[38;5;241m0\u001b[39m)\n\u001b[0;32m    146\u001b[0m scheduler\u001b[38;5;241m.\u001b[39mstep()\n\u001b[0;32m    147\u001b[0m avg_train_loss \u001b[38;5;241m=\u001b[39m running_loss \u001b[38;5;241m/\u001b[39m \u001b[38;5;28mlen\u001b[39m(train_loader\u001b[38;5;241m.\u001b[39mdataset)\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# 셀 4: Train/Val/Test 분할 + 5-Fold 학습/검증 + Test 평가 (샘플링·증강·불균형 보정 + Early Stopping)\n",
    "\n",
    "from sklearn.model_selection import train_test_split, KFold\n",
    "from torch.utils.data import Subset, WeightedRandomSampler\n",
    "from tqdm import tqdm\n",
    "\n",
    "set_seed(42)\n",
    "\n",
    "# (1) 전체 데이터셋 로드 (샘플링 interval=5, transform=None → 분할용)\n",
    "base_dataset = FFPPFrameDataset(\n",
    "    config_path,\n",
    "    frame_interval=5,   # 프레임 샘플링 비율\n",
    "    transform=None      # 분할 통계만 필요하므로 transform 생략\n",
    ")\n",
    "N = len(base_dataset)\n",
    "print(f\"Total sampled samples (alltype): {N}\")\n",
    "\n",
    "# (2) 레이블 배열/분포 확인 (이미 셀 3에서 했지만, 다시 한 번)\n",
    "all_labels = list(base_dataset.labels)\n",
    "unique, counts = np.unique(np.array(all_labels), return_counts=True)\n",
    "print(f\"전체 레이블 분포: {dict(zip(unique, counts))} (0=real,1=fake)\")\n",
    "\n",
    "# (3) 80:20 Train+Val / Test 분할 (stratify)\n",
    "all_indices = list(range(N))\n",
    "trainval_idx, test_idx = train_test_split(\n",
    "    all_indices,\n",
    "    test_size=0.2,\n",
    "    random_state=42,\n",
    "    stratify=all_labels\n",
    ")\n",
    "print(f\"Train+Val count: {len(trainval_idx)}, Test count: {len(test_idx)}\")\n",
    "\n",
    "# (4) Train+Val(80%) → Train(70%) / Val(10%) 분할 (stratify)\n",
    "train_idx, val_idx = train_test_split(\n",
    "    trainval_idx,\n",
    "    test_size=0.125,  # 0.125 * 0.8 = 0.10 전체\n",
    "    random_state=42,\n",
    "    stratify=[all_labels[i] for i in trainval_idx]\n",
    ")\n",
    "print(f\"Train count: {len(train_idx)}, Val count: {len(val_idx)}, Test count: {len(test_idx)}\")\n",
    "\n",
    "# (5) 클래스 불균형 보정을 위한 가중치 계산\n",
    "counts_train_labels = [all_labels[i] for i in train_idx]\n",
    "unique_tr, counts_tr = np.unique(np.array(counts_train_labels), return_counts=True)\n",
    "class_counts = dict(zip(unique_tr, counts_tr))  # {0: real_count, 1: fake_count}\n",
    "total_train = sum(class_counts.values())\n",
    "# 가중치 = 전체 샘플 수 / (클래스별 샘플 수)\n",
    "weight_per_label = {\n",
    "    0: total_train / class_counts[0],\n",
    "    1: total_train / class_counts[1]\n",
    "}\n",
    "print(f\"Train set 클래스별 카운트: {class_counts}, 가중치: {weight_per_label}\")\n",
    "\n",
    "# (6) 5-Fold 학습/검증\n",
    "k_folds = cfg[\"training\"].get(\"k_folds\", 5)\n",
    "kfold = KFold(n_splits=k_folds, shuffle=True, random_state=42)\n",
    "\n",
    "# Early Stopping 기준\n",
    "patience = 3  # Validation AUC가 3회 연속 개선되지 않으면 멈춤\n",
    "\n",
    "# 학습/검증 loop\n",
    "for fold, (tr_sub, va_sub) in enumerate(kfold.split(train_idx)):\n",
    "    print(f\"\\n--- Fold {fold+1}/{k_folds} ---\")\n",
    "    # (A) 실제 Train/Val 인덱스 매핑\n",
    "    true_train_idx = [train_idx[i] for i in tr_sub]\n",
    "    true_val_idx   = [train_idx[i] for i in va_sub]\n",
    "\n",
    "    # (B) Dataset 객체 생성 (샘플링 interval=5 + transform 적용 + 부분 인덱스)\n",
    "    train_dataset = FFPPFrameDataset(\n",
    "        config_path,\n",
    "        frame_interval=5,\n",
    "        transform=train_transform,    # 학습용 증강\n",
    "        indices=true_train_idx\n",
    "    )\n",
    "    val_dataset   = FFPPFrameDataset(\n",
    "        config_path,\n",
    "        frame_interval=5,\n",
    "        transform=val_transform,      # 검증용 최소 전처리\n",
    "        indices=true_val_idx\n",
    "    )\n",
    "\n",
    "    # (C) WeightedRandomSampler 구성 (클래스 불균형 보정)\n",
    "    train_labels_for_sampler = [train_dataset.labels[i] for i in range(len(train_dataset))]\n",
    "    sample_weights = [weight_per_label[label] for label in train_labels_for_sampler]\n",
    "    sampler = WeightedRandomSampler(\n",
    "        sample_weights,\n",
    "        num_samples=len(sample_weights),\n",
    "        replacement=True\n",
    "    )\n",
    "\n",
    "    # (D) DataLoader 구성\n",
    "    train_loader = DataLoader(\n",
    "        train_dataset,\n",
    "        batch_size=cfg[\"batch_size\"],\n",
    "        sampler=sampler,            # shuffle=False 대신 sampler 사용\n",
    "        num_workers=cfg[\"num_workers\"],\n",
    "        pin_memory=True\n",
    "    )\n",
    "    val_loader = DataLoader(\n",
    "        val_dataset,\n",
    "        batch_size=cfg[\"batch_size\"],\n",
    "        shuffle=False,\n",
    "        num_workers=cfg[\"num_workers\"],\n",
    "        pin_memory=True\n",
    "    )\n",
    "\n",
    "    # (E) 모델/손실함수/옵티/스케줄러 설정\n",
    "    model = DeepfakeAdapter(cfg).to(device)\n",
    "    # CrossEntropyLoss 대신 class_weights 적용\n",
    "    class_weights_tensor = torch.tensor([weight_per_label[0], weight_per_label[1]],\n",
    "                                        dtype=torch.float).to(device)\n",
    "    criterion = torch.nn.CrossEntropyLoss(weight=class_weights_tensor)\n",
    "    optimizer = torch.optim.AdamW(\n",
    "        model.parameters(),\n",
    "        lr=float(cfg[\"optimizer\"][\"lr\"]),\n",
    "        weight_decay=float(cfg[\"optimizer\"][\"weight_decay\"])\n",
    "    )\n",
    "    scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(\n",
    "        optimizer, T_max=int(cfg[\"scheduler\"][\"T_max\"])\n",
    "    )\n",
    "\n",
    "    # (F) Fold별 체크포인트/로그 디렉터리 생성\n",
    "    fold_ckpt_dir = os.path.join(cfg[\"output_dir\"], f\"fold_{fold+1}\")\n",
    "    fold_log_dir  = os.path.join(cfg[\"log_dir\"],  f\"fold_{fold+1}\")\n",
    "    os.makedirs(fold_ckpt_dir, exist_ok=True)\n",
    "    os.makedirs(fold_log_dir, exist_ok=True)\n",
    "\n",
    "    writer = SummaryWriter(log_dir=fold_log_dir)\n",
    "    best_auc = 0.0\n",
    "    epochs_since_improve = 0\n",
    "\n",
    "    max_epochs = int(cfg[\"training\"][\"epochs\"])\n",
    "    for epoch in range(max_epochs):\n",
    "        # (G-1) Training 단계\n",
    "        model.train()\n",
    "        running_loss = 0.0\n",
    "        for images, labels in tqdm(train_loader,\n",
    "                                   desc=f\"Fold {fold+1} Train Epoch {epoch+1}\",\n",
    "                                   leave=False):\n",
    "            images = images.to(device)\n",
    "            labels = labels.to(device)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            logits = model(images)\n",
    "            loss = criterion(logits, labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            running_loss += loss.item() * images.size(0)\n",
    "\n",
    "        scheduler.step()\n",
    "        avg_train_loss = running_loss / len(train_loader.dataset)\n",
    "        writer.add_scalar(\"Loss/train\", avg_train_loss, epoch)\n",
    "\n",
    "        # (G-2) Validation 단계\n",
    "        model.eval()\n",
    "        all_labels_fold = []\n",
    "        all_scores_fold = []\n",
    "        with torch.no_grad():\n",
    "            for images, labels in tqdm(val_loader,\n",
    "                                       desc=f\"Fold {fold+1} Val Epoch {epoch+1}\",\n",
    "                                       leave=False):\n",
    "                images = images.to(device)\n",
    "                labels = labels.to(device)\n",
    "                logits = model(images)\n",
    "                probs = torch.softmax(logits, dim=1)[:, 1]  # fake 확률\n",
    "                all_labels_fold.extend(labels.cpu().numpy())\n",
    "                all_scores_fold.extend(probs.cpu().numpy())\n",
    "\n",
    "        val_auc = compute_auc(np.array(all_labels_fold), np.array(all_scores_fold))\n",
    "        val_eer = compute_eer(np.array(all_labels_fold), np.array(all_scores_fold))\n",
    "        writer.add_scalar(\"AUC/val\", val_auc, epoch)\n",
    "        writer.add_scalar(\"EER/val\", val_eer, epoch)\n",
    "\n",
    "        print(\n",
    "            f\"[Fold {fold+1}/{k_folds} | Epoch {epoch+1}/{max_epochs}] \"\n",
    "            f\"Train Loss: {avg_train_loss:.4f}  Val AUC: {val_auc:.4f}  Val EER: {val_eer:.4f}\"\n",
    "        )\n",
    "\n",
    "        # (G-3) 최적 모델 저장 및 Early Stopping 체크\n",
    "        if val_auc > best_auc:\n",
    "            best_auc = val_auc\n",
    "            epochs_since_improve = 0\n",
    "            ckpt = {\n",
    "                \"epoch\": epoch,\n",
    "                \"model_state_dict\": model.state_dict(),\n",
    "                \"optimizer_state_dict\": optimizer.state_dict(),\n",
    "                \"auc\": val_auc,\n",
    "            }\n",
    "            save_path = os.path.join(fold_ckpt_dir, \"best.pth\")\n",
    "            torch.save(ckpt, save_path)\n",
    "            print(f\"→ New best model for Fold {fold+1} saved at {save_path}\")\n",
    "        else:\n",
    "            epochs_since_improve += 1\n",
    "\n",
    "        # patience Epoch 연속 개선 없으면 조기 종료\n",
    "        if epochs_since_improve >= patience:\n",
    "            print(f\"→ Fold {fold+1}: {patience} Epoch 연속 Val AUC 개선 없음 → 조기 종료 (Epoch {epoch+1})\")\n",
    "            break\n",
    "\n",
    "    writer.close()\n",
    "\n",
    "# (H) 5-Fold 완료 후 Test 셋 평가 (transform=val_transform)\n",
    "print(\"\\n=== K-Fold Training Complete ===\")\n",
    "print(\"Evaluating on the Test set using Fold 5's best model...\")\n",
    "\n",
    "test_dataset = FFPPFrameDataset(\n",
    "    config_path,\n",
    "    frame_interval=5,\n",
    "    transform=val_transform,\n",
    "    indices=test_idx\n",
    ")\n",
    "test_loader  = DataLoader(\n",
    "    test_dataset,\n",
    "    batch_size=cfg[\"batch_size\"],\n",
    "    shuffle=False,\n",
    "    num_workers=cfg[\"num_workers\"],\n",
    "    pin_memory=True\n",
    ")\n",
    "print(f\"Test samples: {len(test_dataset)}\")\n",
    "\n",
    "last_ckpt_path = os.path.join(cfg[\"output_dir\"], f\"fold_{k_folds}\", \"best.pth\")\n",
    "assert os.path.exists(last_ckpt_path), f\"Checkpoint not found: {last_ckpt_path}\"\n",
    "\n",
    "model = DeepfakeAdapter(cfg).to(device)\n",
    "ckpt = torch.load(last_ckpt_path, map_location=device)\n",
    "model.load_state_dict(ckpt[\"model_state_dict\"])\n",
    "# (H) Test 평가\n",
    "model.eval()\n",
    "all_test_labels = []\n",
    "all_test_scores = []\n",
    "with torch.no_grad():  # Test 루프도 no_grad로 묶기\n",
    "    for images, labels in tqdm(test_loader, desc=\"Test Evaluation\", leave=False):\n",
    "        images = images.to(device)\n",
    "        labels = labels.to(device)\n",
    "        logits = model(images)\n",
    "        probs = torch.softmax(logits, dim=1)[:, 1]\n",
    "        all_test_labels.extend(labels.cpu().numpy())\n",
    "        all_test_scores.extend(probs.cpu().numpy())\n",
    "\n",
    "test_auc = compute_auc(np.array(all_test_labels), np.array(all_test_scores))\n",
    "test_eer = compute_eer(np.array(all_test_labels), np.array(all_test_scores))\n",
    "print(f\"Final Test AUC: {test_auc:.4f}, Test EER: {test_eer:.4f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16421d86",
   "metadata": {},
   "source": [
    "## 추론(Inference)\n",
    "\n",
    "- 학습된 Fold 5 모델(`fold_5/best.pth`)을 사용하여 **단일 비디오**에 대해 Deepfake 확률을 예측하고,  \n",
    "  결과를 프레임 단위 JSON 파일(`outputs/predictions/[video_name].json`)로 저장합니다.\n",
    "- 과정:\n",
    "  1. `cv2.VideoCapture`로 비디오를 열어 `temp_frames_infer/` 폴더에 프레임 저장  \n",
    "  2. 저장된 프레임에 대해 `get_ffpp_transforms(cfg[\"input_size\"])` 적용 후 모델 예측  \n",
    "  3. 각 프레임별 fake 확률을 JSON으로 저장\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc884d02",
   "metadata": {},
   "outputs": [
    {
     "ename": "AssertionError",
     "evalue": "Video not found: ../data/raw_videos/sample_video.mp4",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAssertionError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[4], line 5\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m# 셀 5: 추론(Inference)\u001b[39;00m\n\u001b[0;32m      2\u001b[0m \n\u001b[0;32m      3\u001b[0m \u001b[38;5;66;03m# (A) 추론할 비디오 파일 경로 지정 (본인이 테스트할 비디오 경로로 수정)\u001b[39;00m\n\u001b[0;32m      4\u001b[0m test_video_path \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m../data/raw_videos/sample_video.mp4\u001b[39m\u001b[38;5;124m\"\u001b[39m  \u001b[38;5;66;03m# 예시\u001b[39;00m\n\u001b[1;32m----> 5\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mexists(test_video_path), \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mVideo not found: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtest_video_path\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m      7\u001b[0m \u001b[38;5;66;03m# (B) 임시 폴더에 프레임 추출\u001b[39;00m\n\u001b[0;32m      8\u001b[0m temp_frames_dir \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtemp_frames_infer\u001b[39m\u001b[38;5;124m\"\u001b[39m\n",
      "\u001b[1;31mAssertionError\u001b[0m: Video not found: ../data/raw_videos/sample_video.mp4"
     ]
    }
   ],
   "source": [
    "# 셀 5: 추론(Inference)\n",
    "\n",
    "# (A) 추론할 비디오 파일 경로 지정 (본인이 테스트할 비디오 경로로 수정)\n",
    "test_video_path = \"../data/raw_videos/sample_video.mp4\"  # 예시\n",
    "assert os.path.exists(test_video_path), f\"Video not found: {test_video_path}\"\n",
    "\n",
    "# (B) 임시 폴더에 프레임 추출\n",
    "temp_frames_dir = \"temp_frames_infer\"\n",
    "if os.path.exists(temp_frames_dir):\n",
    "    for f in os.listdir(temp_frames_dir):\n",
    "        os.remove(os.path.join(temp_frames_dir, f))\n",
    "else:\n",
    "    os.makedirs(temp_frames_dir)\n",
    "\n",
    "cap = cv2.VideoCapture(test_video_path)\n",
    "frame_idx = 0\n",
    "while True:\n",
    "    ret, frame = cap.read()\n",
    "    if not ret:\n",
    "        break\n",
    "    save_path = os.path.join(temp_frames_dir, f\"frame_{frame_idx:06d}.jpg\")\n",
    "    cv2.imwrite(save_path, frame)\n",
    "    frame_idx += 1\n",
    "cap.release()\n",
    "print(f\"Extracted {frame_idx} frames to '{temp_frames_dir}'\")\n",
    "\n",
    "# (C) TempFrameDataset 정의 및 DataLoader 구성\n",
    "transform = get_ffpp_transforms(cfg[\"input_size\"])\n",
    "frame_paths = sorted([\n",
    "    os.path.join(temp_frames_dir, fn)\n",
    "    for fn in os.listdir(temp_frames_dir)\n",
    "    if fn.endswith(\".jpg\") or fn.endswith(\".png\")\n",
    "])\n",
    "\n",
    "class TempFrameDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, paths, transform):\n",
    "        self.paths = paths\n",
    "        self.transform = transform\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.paths)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        img = Image.open(self.paths[idx]).convert(\"RGB\")\n",
    "        return self.transform(img), self.paths[idx]\n",
    "\n",
    "temp_dataset = TempFrameDataset(frame_paths, transform)\n",
    "temp_loader = DataLoader(\n",
    "    temp_dataset,\n",
    "    batch_size=cfg[\"batch_size\"],\n",
    "    shuffle=False,\n",
    "    num_workers=cfg[\"num_workers\"],\n",
    "    pin_memory=True\n",
    ")\n",
    "\n",
    "# (D) 마지막 Fold 모델 로드\n",
    "best_ckpt_path = os.path.join(cfg[\"output_dir\"], f\"fold_{cfg['training']['k_folds']}\", \"best.pth\")\n",
    "assert os.path.exists(best_ckpt_path), f\"Checkpoint not found: {best_ckpt_path}\"\n",
    "\n",
    "model = DeepfakeAdapter(cfg).to(device)\n",
    "ckpt = torch.load(best_ckpt_path, map_location=device)\n",
    "model.load_state_dict(ckpt[\"model_state_dict\"])\n",
    "model.eval()\n",
    "\n",
    "# (E) 프레임 단위 예측\n",
    "results = {}  # { \"frame_path\": fake_probability }\n",
    "with torch.no_grad():\n",
    "    for imgs, paths in temp_loader:\n",
    "        imgs = imgs.to(device)\n",
    "        logits = model(imgs)\n",
    "        probs = torch.softmax(logits, dim=1)[:, 1].cpu().numpy()  # fake 확률\n",
    "        for p, prob in zip(paths, probs):\n",
    "            results[p] = float(prob)\n",
    "\n",
    "# (F) JSON으로 저장\n",
    "video_name = os.path.splitext(os.path.basename(test_video_path))[0]\n",
    "output_json_dir = \"outputs/predictions\"\n",
    "os.makedirs(output_json_dir, exist_ok=True)\n",
    "output_json_path = os.path.join(output_json_dir, f\"{video_name}.json\")\n",
    "\n",
    "with open(output_json_path, \"w\") as f:\n",
    "    json.dump(results, f, indent=2)\n",
    "\n",
    "print(f\"Saved prediction results to '{output_json_path}'\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af00544d",
   "metadata": {},
   "source": [
    "## 결과 시각화 예시\n",
    "\n",
    "- **추출된 프레임 중 상위 5개**를 선택하여,  \n",
    "  프레임 원본 이미지 + 예측된 “Fake 확률”을 함께 시각화해 봅니다.\n",
    "- JSON 파일에서 상위 5개 확률 프레임을 골라서 확인합니다.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f4254dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 셀 6: 결과 시각화 예시\n",
    "\n",
    "with open(output_json_path, \"r\") as f:\n",
    "    results_json = json.load(f)\n",
    "\n",
    "# 프레임별 확률 내림차순 정렬 → 상위 5개\n",
    "sorted_items = sorted(results_json.items(), key=lambda x: x[1], reverse=True)\n",
    "top5 = sorted_items[:5]\n",
    "\n",
    "fig, axes = plt.subplots(1, 5, figsize=(20, 4))\n",
    "for i, (frame_path, prob) in enumerate(top5):\n",
    "    img = Image.open(frame_path).convert(\"RGB\")\n",
    "    axes[i].imshow(np.array(img))\n",
    "    axes[i].set_title(f\"{os.path.basename(frame_path)}\\nFake: {prob:.3f}\")\n",
    "    axes[i].axis(\"off\")\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "477bd3cc",
   "metadata": {},
   "source": [
    "## 마무리 및 토의\n",
    "\n",
    "1. **데이터 로드 및 전처리 확인**  \n",
    "   - `FFPPFrameDataset`을 통해 **alltype** 폴더 구조에서 모든 프레임과 Real/Fake 레이블을 올바르게 로드했음을 확인했습니다.\n",
    "\n",
    "2. **Train(70%) / Val(10%) / Test(20%) 분할**  \n",
    "   - 전체 데이터를 80:20으로 분할한 뒤, Train(80%) 내부를 다시 7:1로 분할하여 **Train 70%, Val 10%, Test 20%** 비율을 만들었습니다.\n",
    "\n",
    "3. **5-Fold Cross-Validation 학습 + 검증**  \n",
    "   - `KFold(n_splits=5, shuffle=True, random_state=42)`로 Train(70%)을 5개 Fold로 나누어,  \n",
    "   - 각 Fold마다 학습 및 검증을 수행하고 **Fold별 최적 모델(best.pth)** 을 저장했습니다.\n",
    "\n",
    "4. **Test(20%) 최종 평가**  \n",
    "   - 마지막(5번째) Fold의 최적 모델을 불러와 Test(20%) 데이터셋에 대해 **AUC/EER**을 계산했습니다.\n",
    "\n",
    "5. **추론(Inference)**  \n",
    "   - 새로운 비디오 파일을 “영상 → 프레임”으로 분할한 뒤,  \n",
    "   - 마지막 Fold 모델로 프레임별 Deepfake 확률을 예측하여 JSON(`outputs/predictions/[video_name].json`)으로 저장했습니다.\n",
    "\n",
    "6. **결과 시각화**  \n",
    "   - 상위 5개 “Fake 확률이 높은 프레임”을 시각화하여,  \n",
    "     모델이 어느 순간 가장 Fake이라고 판단했는지 직관적으로 확인했습니다.\n",
    "\n",
    "---\n",
    "\n",
    "### 향후 방향\n",
    "\n",
    "- **하이퍼파라미터 튜닝**  \n",
    "  - `training.k_folds`, `optimizer.lr`, `scheduler.T_max`, `model.adapter_dim` 등을 변경하며 성능 최적화  \n",
    "  - 5-Fold 학습 결과를 앙상블(Ensembling)하여 Test 성능 향상 시도 가능\n",
    "\n",
    "- **Transformer-XAI**  \n",
    "  - 현재는 ViT + Dual-Level Adapter 기반 분류기까지만 구현되어 있습니다.  \n",
    "  - 이후 “Attention Map”, “Token Attribution” 등 Transformer 설명 기법을 추가하여,  \n",
    "    모델의 의사결정 근거를 시각화할 예정입니다.\n",
    "\n",
    "- **배포 및 서비스**  \n",
    "  학습된 모델(`fold_i/best.pth`)을 REST API 또는 Streamlit 앱으로 배포하여,  \n",
    "  사용자가 업로드한 동영상에 대해 실시간으로 Deepfake 여부를 확인할 수 있도록 서비스화할 수 있습니다.\n",
    "\n",
    "---\n",
    "\n",
    "이 노트북이 FFPP 기반 DeepFake Detection 전체 파이프라인을 이해하고, 직접 실험해보는 데 도움이 되길 바랍니다. 감사합니다!\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dfML",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
